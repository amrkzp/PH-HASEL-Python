Look back: 400
Epochs: 250
Train ratio: 0.8
Batch size: 40000
Learning rate: 0.01
Weight Decay: 1e-05
Momentum: 0.9
#ADAM OPTIMIZER!
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
├─Flatten: 1-1                           [-1, 800]                 --
├─Sequential: 1-2                        [-1, 1]                   --
|    └─Linear: 2-1                       [-1, 64]                  51,264
|    └─ReLU: 2-2                         [-1, 64]                  --
|    └─BatchNorm1d: 2-3                  [-1, 64]                  128
|    └─Linear: 2-4                       [-1, 64]                  4,160
|    └─ReLU: 2-5                         [-1, 64]                  --
|    └─BatchNorm1d: 2-6                  [-1, 64]                  128
|    └─Linear: 2-7                       [-1, 64]                  4,160
|    └─ReLU: 2-8                         [-1, 64]                  --
|    └─BatchNorm1d: 2-9                  [-1, 64]                  128
|    └─Linear: 2-10                      [-1, 64]                  4,160
|    └─ReLU: 2-11                        [-1, 64]                  --
|    └─BatchNorm1d: 2-12                 [-1, 64]                  128
|    └─Linear: 2-13                      [-1, 1]                   65
|    └─Sigmoid: 2-14                     [-1, 1]                   --
==========================================================================================
Total params: 64,321
Trainable params: 64,321
Non-trainable params: 0
Total mult-adds (M): 0.13
==========================================================================================
Input size (MB): 106.20
Forward/backward pass size (MB): 0.00
Params size (MB): 0.25
Estimated Total Size (MB): 106.45
==========================================================================================